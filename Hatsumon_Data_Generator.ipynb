{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec6a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6efec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HatsumonDataGenerator:\n",
    "    def __init__(self, path: str, tokenizer):\n",
    "        self.two_way = {\"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1, \"5\": 2, \"6\": 2, \"7\": 2}\n",
    "        self.four_way = {\"1\": 1, \"2\": 1, \"3\": 2, \"4\": 2, \"5\": 3, \"6\": 3, \"7\": 4}\n",
    "        self.seven_way = {\"1\": 1, \"2\": 2, \"3\": 3, \"4\": 4, \"5\": 5, \"6\": 6, \"7\": 7}\n",
    "        self.bert_tokenizer = tokenizer\n",
    "        self.json_path = path\n",
    "        self.dataset = self.load_json_file(self.json_path)\n",
    "        \n",
    "    def load_json_file(self, path: str) -> dict:\n",
    "        with open(path, \"r\") as j:\n",
    "            dataset = json.load(j)\n",
    "        return dataset\n",
    "    \n",
    "    def view_all_data(self, way_type: str) -> pd.DataFrame:\n",
    "        question_numbers = []\n",
    "        questions = []\n",
    "        model_anss = []\n",
    "        edu_anss = []\n",
    "        labels = []\n",
    "        \n",
    "        for study_number in self.dataset.keys():\n",
    "            for question_number in self.dataset[study_number].keys():\n",
    "                edu_num = len(self.dataset[study_number][question_number][\"edu_ans\"])\n",
    "                q_numbert = study_number + \"-\" + question_number\n",
    "                question = self.dataset[study_number][question_number][\"question\"]\n",
    "                model_ans = self.dataset[study_number][question_number][\"model_ans\"]\n",
    "                \n",
    "                question_numbers.extend([q_numbert] * edu_num)\n",
    "                questions.extend([question] * edu_num)\n",
    "                model_anss.extend([model_ans] * edu_num)\n",
    "                \n",
    "                for edu_ans in self.dataset[study_number][question_number][\"edu_ans\"]:\n",
    "                    ans = edu_ans[\"ans\"]\n",
    "                    label = edu_ans[\"label\"]\n",
    "                    edu_anss.append(ans)\n",
    "                    labels.append(label)\n",
    "                    \n",
    "        pd_question_numbers = pd.DataFrame(np.array(question_numbers, dtype = str), columns=[\"Question number\"])\n",
    "        pd_questions = pd.DataFrame(np.array(questions, dtype = str), columns=[\"Question\"])\n",
    "        pd_model_anss = pd.DataFrame(np.array(model_anss, dtype = str), columns=[\"Model answer\"])\n",
    "        pd_edu_anss = pd.DataFrame(np.array(edu_anss, dtype = str), columns=[\"Edu answer\"])\n",
    "        pd_labels = pd.DataFrame(np.array(labels, dtype = str), columns=[\"Label\"])\n",
    "        \n",
    "        pd_all_data = pd.concat([pd_question_numbers, pd_questions, pd_model_anss, pd_edu_anss, pd_labels], axis=1)\n",
    "        \n",
    "        return pd_all_data\n",
    "                \n",
    "                \n",
    "            \n",
    "    def get_TUA(self, way_type: str) -> list:\n",
    "        num_cross_val = 5\n",
    "        train_pairs = np.zeros((num_cross_val, 1), dtype = str)\n",
    "        train_labels = np.zeros((num_cross_val, 1), dtype = str)\n",
    "        test_pairs = np.zeros((num_cross_val, 1), dtype = str)\n",
    "        test_labels = np.zeros((num_cross_val, 1), dtype = str)\n",
    "        \n",
    "        for study_number in self.dataset.keys():\n",
    "            for question_number in self.dataset[study_number].keys():\n",
    "                question = self.dataset[study_number][question_number][\"question\"]\n",
    "                model_ans = self.dataset[study_number][question_number][\"model_ans\"]\n",
    "                \n",
    "                pairs = []\n",
    "                labels = []\n",
    "                for edu_ans in self.dataset[study_number][question_number][\"edu_ans\"]:\n",
    "                    ans = edu_ans[\"ans\"]\n",
    "                    pair = question + self.bert_tokenizer.tokenizer.sep_token + model_ans + self.bert_tokenizer.tokenizer.sep_token + ans\n",
    "                    label = edu_ans[\"label\"]\n",
    "                    pairs.append(pair)\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                pairs = [pairs] * num_cross_val  # pairs = [num_cross_val, len(edu_ans)]\n",
    "                labels = [labels] * num_cross_val # labelss = [num_cross_val, len(labels)]\n",
    "                splitted_pairs = np.array(list(map(self.split_train_test_for_TUA, list(enumerate(pairs)))), dtype = str)\n",
    "                splitted_labels = np.array(list(map(self.split_train_test_for_TUA, list(enumerate(labels)))), dtype = str)\n",
    "                train_pairs = np.hstack((train_pairs, splitted_pairs[:, :-1])) # dim=1にどんどんスタックしていく\n",
    "                train_labels = np.hstack((train_labels, splitted_labels[:, :-1]))\n",
    "                test_pairs = np.hstack((test_pairs, splitted_pairs[:, -1].reshape(num_cross_val, 1)))\n",
    "                test_labels = np.hstack((test_labels, splitted_labels[:, -1].reshape(num_cross_val, 1)))\n",
    "                \n",
    "        train_dataset = self.encording(train_pairs, train_labels, way_type)\n",
    "        test_dataset = self.encording(test_pairs, test_labels, way_type)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "                \n",
    "            \n",
    "        \n",
    "    def get_TUQ(self, way_type: str) -> list:\n",
    "        num_cross_val = 5\n",
    "        train_pairs = np.zeros((num_cross_val, 1), dtype = str)\n",
    "        train_labels = np.zeros((num_cross_val, 1), dtype = str)\n",
    "        test_pairs = np.zeros((num_cross_val, 1), dtype = str)\n",
    "        test_labels = np.zeros((num_cross_val, 1), dtype = str)\n",
    "        \n",
    "        for study_number in self.dataset.keys():\n",
    "            study_pairs = [] #study_pairs[i] = Question_i\n",
    "            study_labels = [] #study_labels[i] = Question_i\n",
    "            for question_number in self.dataset[study_number].keys():\n",
    "                question = self.dataset[study_number][question_number][\"question\"]\n",
    "                model_ans = self.dataset[study_number][question_number][\"model_ans\"]\n",
    "                \n",
    "                pairs = []\n",
    "                labels = []\n",
    "                for edu_ans in self.dataset[study_number][question_number][\"edu_ans\"]:\n",
    "                    ans = edu_ans[\"ans\"]\n",
    "                    pair = question + self.bert_tokenizer.tokenizer.sep_token + model_ans + self.bert_tokenizer.tokenizer.sep_token + ans\n",
    "                    label = edu_ans[\"label\"]\n",
    "                    pairs.append(pair)\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                study_pairs.append(pairs)\n",
    "                study_labels.append(labels)\n",
    "                \n",
    "            study_pairs = [study_pairs] * num_cross_val\n",
    "            study_labels = [study_labels] * num_cross_val\n",
    "            \n",
    "            np_train_pairs, np_train_labels, np_test_pairs, np_test_labels = self.split_train_test_for_TUQ(study_pairs, study_labels)\n",
    "            train_pairs = np.hstack((train_pairs, np_train_pairs))\n",
    "            train_labels = np.hstack((train_labels, np_train_labels))\n",
    "            test_pairs = np.hstack((test_pairs, np_test_pairs))\n",
    "            test_labels = np.hstack((test_labels, np_test_labels))                \n",
    "        \n",
    "        train_dataset = self.encording(train_pairs, train_labels, way_type)\n",
    "        test_dataset = self.encording(test_pairs, test_labels, way_type)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    \n",
    "    def split_train_test_for_TUA(self, data_add_index: list) -> list:\n",
    "        index = data_add_index[0]\n",
    "        data = data_add_index[1]\n",
    "        \n",
    "        if len(data) <= index:\n",
    "            random.shuffle(data)\n",
    "            testset = data[-1]\n",
    "            trainset = data[:-1]\n",
    "        else:\n",
    "            testset = data[index]\n",
    "            trainset = data[:index] + data[index+1:]\n",
    "        \n",
    "        data = trainset + [testset]\n",
    "            \n",
    "        return data  # data[:-1] = train_data,  data[-1] = test_data\n",
    "    \n",
    "    \n",
    "    def split_train_test_for_TUQ(self, pairs: list, labels: list) -> list:\n",
    "        test_pairs, train_pairs = [pair[i] for i, pair in enumerate(pairs)], [pair[:i]+pair[i+1:] for i, pair in enumerate(pairs)]\n",
    "        test_labels, train_labels = [label[i] for i, label in enumerate(labels)], [label[:i]+label[i+1:] for i, label in enumerate(labels)]\n",
    "        \n",
    "        np_test_pairs = np.array(test_pairs, dtype = str)\n",
    "        np_test_labels = np.array(test_labels, dtype = str)\n",
    "        \n",
    "        train_pairs = np.array(train_pairs, dtype = str)\n",
    "        train_labels = np.array(train_labels, dtype = str)\n",
    "        np_train_pairs = train_pairs.reshape(train_pairs.shape[0], train_pairs.shape[1]*train_pairs.shape[2])\n",
    "        np_train_labels = train_labels.reshape(train_labels.shape[0], train_labels.shape[1]*train_labels.shape[2])\n",
    "                \n",
    "        return np_train_pairs, np_train_labels, np_test_pairs, np_test_labels\n",
    "            \n",
    "            \n",
    "            \n",
    "        return TUQset\n",
    "    \n",
    "    \n",
    "    def encording(self, pairs: np.ndarray, labels: np.ndarray, way_type: str) -> list:\n",
    "        #np.zeros((num_cross_val, 1), dtype = str)の分を削除\n",
    "        pairs = np.delete(pairs, 0, 1).tolist()\n",
    "        labels = np.delete(labels, 0, 1).tolist()\n",
    "        \n",
    "        pairs = list(map(self.bert_tokenizer.encode, pairs))\n",
    "        labels = list(map(self.replace_way_type, labels, [way_type]*len(labels)))\n",
    "        \n",
    "        dataset = list(map(self.create_dataset, pairs, labels))\n",
    "        return dataset\n",
    "    \n",
    "    def replace_way_type(self, labels: list, way_type: str) -> list:\n",
    "        if way_type == \"2-way\":\n",
    "            labels = list(map(self.two_way.get, labels))\n",
    "        elif way_type == \"4-way\":\n",
    "            labels = list(map(self.four_way.get, labels))\n",
    "        else:\n",
    "            labels = list(map(self.seven_way.get, labels))\n",
    "            \n",
    "        return labels\n",
    "    \n",
    "    def create_dataset(self, pairs: list, labels: list) -> TensorDataset:\n",
    "        \n",
    "        labels = np.array(labels, dtype = int)\n",
    "        labels = labels - 1\n",
    "        dataset = TensorDataset(pairs[\"input_ids\"], pairs[\"attention_mask\"], \n",
    "                            torch.from_numpy(labels))\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c102826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hatsumon_data(zyouhou1_path: str, tokenizer) -> HatsumonDataGenerator:\n",
    "    if not(os.path.exists(zyouhou1_path)):\n",
    "        raise ValueError(\"Cannot find json file. Please run create_dataset_from_csv.py\")\n",
    "    generator = HatsumonDataGenerator(zyouhou1_path, tokenizer)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b42ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./zyouhou1_hatsumon.json\"\n",
    "zd = build_hatsumon_data(path, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041dbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zd.view_all_data(\"2-way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22df5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"./zyouhou1_hatsumon.csv\"\n",
    "a.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2aee517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrameGroupBy.count of <pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f58b9d73790>>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.groupby(\"Label\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1446a334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    181\n",
       "3    112\n",
       "6     90\n",
       "5     38\n",
       "2     14\n",
       "4     13\n",
       "1      2\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb21450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "181+112+90+38+14+13+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da38e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
